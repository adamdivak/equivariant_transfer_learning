============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
05/17/2024 22:32:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
05/17/2024 22:32:35 - INFO - transformers.configuration_utils -   loading configuration file models/bert_regard_v2_large/checkpoint-300/config.json
05/17/2024 22:32:35 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "_num_labels": 4,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/17/2024 22:32:35 - INFO - transformers.tokenization_utils -   Model name 'models/bert_regard_v2_large/checkpoint-300' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'models/bert_regard_v2_large/checkpoint-300' is a path, a model identifier, or url to a directory containing tokenizer files.
05/17/2024 22:32:35 - INFO - transformers.tokenization_utils -   Didn't find file models/bert_regard_v2_large/checkpoint-300/added_tokens.json. We won't load it.
05/17/2024 22:32:35 - INFO - transformers.tokenization_utils -   loading file models/bert_regard_v2_large/checkpoint-300/vocab.txt
05/17/2024 22:32:35 - INFO - transformers.tokenization_utils -   loading file None
05/17/2024 22:32:35 - INFO - transformers.tokenization_utils -   loading file models/bert_regard_v2_large/checkpoint-300/special_tokens_map.json
05/17/2024 22:32:35 - INFO - transformers.tokenization_utils -   loading file models/bert_regard_v2_large/checkpoint-300/tokenizer_config.json
05/17/2024 22:32:36 - INFO - transformers.modeling_utils -   loading weights file models/bert_regard_v2_large/checkpoint-300/pytorch_model.bin
05/17/2024 22:32:50 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='data/regard', device=device(type='cuda'), do_eval=False, do_lower_case=True, do_predict=True, do_train=False, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='models/bert_regard_v2_large/checkpoint-300', model_type='bert', model_version=2, n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='models/bert_regard_v2_large', output_pred_dir='EquiNLG/scored_samples/GPT2', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=50, seed=42, server_ip='', server_port='', test_file='test.tsv', tokenizer_name='', warmup_steps=0, weight_decay=0.0)
05/17/2024 22:32:50 - INFO - transformers.tokenization_utils -   Model name 'models/bert_regard_v2_large' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'models/bert_regard_v2_large' is a path, a model identifier, or url to a directory containing tokenizer files.
05/17/2024 22:32:50 - INFO - transformers.tokenization_utils -   Didn't find file models/bert_regard_v2_large/added_tokens.json. We won't load it.
05/17/2024 22:32:50 - INFO - transformers.tokenization_utils -   Didn't find file models/bert_regard_v2_large/special_tokens_map.json. We won't load it.
05/17/2024 22:32:50 - INFO - transformers.tokenization_utils -   Didn't find file models/bert_regard_v2_large/tokenizer_config.json. We won't load it.
05/17/2024 22:32:50 - INFO - transformers.tokenization_utils -   loading file models/bert_regard_v2_large/vocab.txt
05/17/2024 22:32:50 - INFO - transformers.tokenization_utils -   loading file None
05/17/2024 22:32:50 - INFO - transformers.tokenization_utils -   loading file None
05/17/2024 22:32:50 - INFO - transformers.tokenization_utils -   loading file None
05/17/2024 22:32:50 - INFO - transformers.configuration_utils -   loading configuration file models/bert_regard_v2_large/checkpoint-300/config.json
05/17/2024 22:32:50 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "_num_labels": 4,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/17/2024 22:32:50 - INFO - transformers.modeling_utils -   loading weights file models/bert_regard_v2_large/checkpoint-300/pytorch_model.bin
05/17/2024 22:32:56 - INFO - __main__ -   Loading features from cached file data/regard/cached_test.tsv_checkpoint-300_128
05/17/2024 22:32:56 - INFO - __main__ -   ***** Running evaluation  *****
05/17/2024 22:32:56 - INFO - __main__ -     Num examples = 30
05/17/2024 22:32:56 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "scripts/run_classifier.py", line 703, in <module>
    main()
  File "scripts/run_classifier.py", line 689, in main
    result, predictions = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=test_file, is_test=True)
  File "scripts/run_classifier.py", line 296, in evaluate
    outputs = model(**inputs)
  File "/home/scur0395/.conda/envs/biases/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/scur0395/.conda/envs/biases/lib/python3.7/site-packages/transformers/modeling_bert.py", line 1142, in forward
    inputs_embeds=inputs_embeds,
  File "/home/scur0395/.conda/envs/biases/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/scur0395/.conda/envs/biases/lib/python3.7/site-packages/transformers/modeling_bert.py", line 734, in forward
    encoder_attention_mask=encoder_extended_attention_mask,
  File "/home/scur0395/.conda/envs/biases/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/scur0395/.conda/envs/biases/lib/python3.7/site-packages/transformers/modeling_bert.py", line 407, in forward
    hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask
  File "/home/scur0395/.conda/envs/biases/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/scur0395/.conda/envs/biases/lib/python3.7/site-packages/transformers/modeling_bert.py", line 368, in forward
    self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
  File "/home/scur0395/.conda/envs/biases/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/scur0395/.conda/envs/biases/lib/python3.7/site-packages/transformers/modeling_bert.py", line 314, in forward
    hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask
  File "/home/scur0395/.conda/envs/biases/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/scur0395/.conda/envs/biases/lib/python3.7/site-packages/transformers/modeling_bert.py", line 234, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
RuntimeError: cublas runtime error : the GPU program failed to execute at /tmp/pip-req-build-58y_cjjl/aten/src/THC/THCBlas.cu:331

JOB STATISTICS
==============
Job ID: 6271760
Cluster: snellius
User/Group: scur0395/scur0395
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:31
CPU Efficiency: 4.31% of 00:12:00 core-walltime
Job Wall-clock time: 00:00:40
Memory Utilized: 1.21 MB
Memory Efficiency: 0.00% of 120.00 GB

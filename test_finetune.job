#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=1
#SBATCH --job-name=MNIST_eq
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --time=00:50:00


module purge
module load 2022
module load Anaconda3/2022.05

# activate the environment
source activate lambda_equitune

# Arrays of models, datasets, and transformations
# models=('RN50' 'RN101' 'ViT-B/32' 'ViT-B/16')
# datasets=("ImagenetV2" "CIFAR100")
# transformations=("" "flip" "rot90")

# Loop over each model, dataset, and transformation
# for model in "${models[@]}"; do
#     for dataset in "${datasets[@]}"; do
#         for transformation in "${transformations[@]}"; do
#             # Run your code with the current parameters
#             srun python EquiCLIP/main_equizero.py --method "vanilla" --model "${model}" --dataset_name "${dataset}" --data_transformations "${transformation}"
#         done
#     done
# done

# srun python EquiCLIP/main_finetune.py  --dataset_name ISIC2018  --method equitune --group_name rot90 --data_transformations rot90  --model_name 'RN50' --iter_per_finetune 25 --lr 0.03
srun python EquiCLIP/main_finetune.py  --dataset_name MNIST  --method equitune --group_name rot90 --data_transformations rot90  --model_name 'RN50'